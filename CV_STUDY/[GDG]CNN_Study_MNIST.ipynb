{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyGiNURzmV69"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline Model"
      ],
      "metadata": {
        "id": "Y2ACrbESoIpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "wEBFkyYYmini"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 4"
      ],
      "metadata": {
        "id": "Z19_oj0pm-n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])"
      ],
      "metadata": {
        "id": "VM-taXwlnPgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 7 * 7 * 64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on test images: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5WRvGFCnTM6",
        "outputId": "b3b0ee0b-c55d-4428-8e0b-cdb37d11352c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.61MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.28MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.41MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Step [100/938], Loss: 2.2586\n",
            "Epoch [1/4], Step [200/938], Loss: 2.2021\n",
            "Epoch [1/4], Step [300/938], Loss: 2.1678\n",
            "Epoch [1/4], Step [400/938], Loss: 2.0777\n",
            "Epoch [1/4], Step [500/938], Loss: 1.9391\n",
            "Epoch [1/4], Step [600/938], Loss: 1.8623\n",
            "Epoch [1/4], Step [700/938], Loss: 1.6832\n",
            "Epoch [1/4], Step [800/938], Loss: 1.5051\n",
            "Epoch [1/4], Step [900/938], Loss: 1.2238\n",
            "Epoch [2/4], Step [100/938], Loss: 1.1296\n",
            "Epoch [2/4], Step [200/938], Loss: 0.8622\n",
            "Epoch [2/4], Step [300/938], Loss: 0.7161\n",
            "Epoch [2/4], Step [400/938], Loss: 0.7996\n",
            "Epoch [2/4], Step [500/938], Loss: 0.7273\n",
            "Epoch [2/4], Step [600/938], Loss: 0.5761\n",
            "Epoch [2/4], Step [700/938], Loss: 0.6730\n",
            "Epoch [2/4], Step [800/938], Loss: 0.4870\n",
            "Epoch [2/4], Step [900/938], Loss: 0.3842\n",
            "Epoch [3/4], Step [100/938], Loss: 0.2793\n",
            "Epoch [3/4], Step [200/938], Loss: 0.3769\n",
            "Epoch [3/4], Step [300/938], Loss: 0.6267\n",
            "Epoch [3/4], Step [400/938], Loss: 0.4924\n",
            "Epoch [3/4], Step [500/938], Loss: 0.3002\n",
            "Epoch [3/4], Step [600/938], Loss: 0.4070\n",
            "Epoch [3/4], Step [700/938], Loss: 0.3806\n",
            "Epoch [3/4], Step [800/938], Loss: 0.3089\n",
            "Epoch [3/4], Step [900/938], Loss: 0.5203\n",
            "Epoch [4/4], Step [100/938], Loss: 0.3337\n",
            "Epoch [4/4], Step [200/938], Loss: 0.3999\n",
            "Epoch [4/4], Step [300/938], Loss: 0.2265\n",
            "Epoch [4/4], Step [400/938], Loss: 0.3481\n",
            "Epoch [4/4], Step [500/938], Loss: 0.2527\n",
            "Epoch [4/4], Step [600/938], Loss: 0.4446\n",
            "Epoch [4/4], Step [700/938], Loss: 0.1735\n",
            "Epoch [4/4], Step [800/938], Loss: 0.1719\n",
            "Epoch [4/4], Step [900/938], Loss: 0.5290\n",
            "Test Accuracy of the model on test images: 91.54 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mKNs_7dJoKsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 옵티마이저 Adam + 레이어 및 드랍아웃 추가 + BN 적용 + 손실함수 크로스 엔트로피"
      ],
      "metadata": {
        "id": "PD2aaRZIoRO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 4\n",
        "\n",
        "# MNIST dataset with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "train_loader  = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader   = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Improved CNN model\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 128)  # After 3 pooling operations: 28 → 14 → 7 → 3\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # (28x28 → 14x14)\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # (14x14 → 7x7)\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # (7x7 → 3x3)\n",
        "        x = x.view(-1, 128 * 3 * 3)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = ImprovedCNN().to(device)\n",
        "\n",
        "# Loss + Optimizer + Scheduler\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.7)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPMOOp3CoSfy",
        "outputId": "defbec91-48f5-402f-bc72-17adede4a65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Loss: 0.6032\n",
            "Epoch [2/4], Loss: 0.5832\n",
            "Epoch [3/4], Loss: 0.6138\n",
            "Epoch [4/4], Loss: 0.5686\n",
            "Test Accuracy: 99.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 4\n",
        "\n",
        "# 1. Data Loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader   = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 2. BasicBlock for ResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "# 3. ResNet Class\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64,  num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride):\n",
        "        strides = [stride] + [1] * (blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride=s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "# 4. Model, Loss, Optimizer, Scheduler\n",
        "model = ResNet18().to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.7)\n",
        "\n",
        "# 5. Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 6. Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmYQoDkTo3iT",
        "outputId": "ac4be401-f7c5-45f0-d5db-5bbd361c071d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Loss: 0.5266\n",
            "Epoch [2/4], Loss: 0.5392\n",
            "Epoch [3/4], Loss: 0.5094\n",
            "Epoch [4/4], Loss: 0.5052\n",
            "Test Accuracy: 99.38%\n"
          ]
        }
      ]
    }
  ]
}