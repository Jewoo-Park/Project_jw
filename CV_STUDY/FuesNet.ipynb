{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lC8tnRlXyiF1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# 1. VOCSegmentation 데이터셋 (RGB + Dummy Depth)\n",
        "# ---------------------------------\n",
        "class VOCSegWithDepth(Dataset):\n",
        "    \"\"\"\n",
        "    VOCSegmentation 데이터셋에 대해,\n",
        "    각 샘플을 (RGB image, Dummy Depth image, Target segmentation mask)으로 반환합니다.\n",
        "\n",
        "    Dummy Depth는 RGB 이미지를 그레이스케일로 변환하여 만듭니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, root, year='2012', image_set='train',\n",
        "                 transform_rgb=None, transform_depth=None, transform_target=None, download=False):\n",
        "        self.voc = VOCSegmentation(root=root, year=year, image_set=image_set, download=download)\n",
        "        self.transform_rgb = transform_rgb\n",
        "        self.transform_depth = transform_depth  # Dummy depth transform\n",
        "        self.transform_target = transform_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.voc)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # voc[ index ] returns (image, target) both as PIL images\n",
        "        rgb, target = self.voc[index]\n",
        "\n",
        "        # Dummy Depth: convert RGB to grayscale\n",
        "        depth = TF.to_grayscale(rgb, num_output_channels=1)\n",
        "\n",
        "        if self.transform_rgb:\n",
        "            rgb = self.transform_rgb(rgb)\n",
        "        if self.transform_depth:\n",
        "            depth = self.transform_depth(depth)\n",
        "        if self.transform_target:\n",
        "            target = self.transform_target(target)\n",
        "        return rgb, depth, target"
      ],
      "metadata": {
        "id": "cepvRZ9U2De-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# 2. Transform 정의\n",
        "# ---------------------------------\n",
        "img_size = (224, 224)\n",
        "\n",
        "# RGB 이미지: bilinear interpolation, tensor 변환 및 정규화 (ImageNet 평균/표준편차 사용)\n",
        "transform_rgb = transforms.Compose([\n",
        "    transforms.Resize(img_size, interpolation=Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dummy Depth: Nearest Neighbor로 리사이즈, 단일 채널 Tensor로 변환\n",
        "transform_depth = transforms.Compose([\n",
        "    transforms.Resize(img_size, interpolation=Image.NEAREST),\n",
        "    transforms.ToTensor()  # 출력은 (1, H, W) 범위 [0,1]\n",
        "])\n",
        "\n",
        "# Segmentation target: Nearest Neighbor, Tensor 변환 (VOC segmentation은 255가 ignore index)\n",
        "# VOCSegmentation의 target은 PIL 이미지로, 각 픽셀이 0~21 (또는 0~20) 클래스 값을 가짐.\n",
        "class ToLongTensor(object):\n",
        "    def __call__(self, pic):\n",
        "        return torch.from_numpy(np.array(pic)).long()\n",
        "\n",
        "transform_target = transforms.Compose([\n",
        "    transforms.Resize(img_size, interpolation=Image.NEAREST),\n",
        "    ToLongTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "7V092JsT2PMX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# 3. 데이터셋 및 DataLoader 준비\n",
        "# ---------------------------------\n",
        "root = './VOC2012'  # 저장 경로\n",
        "# 학습 데이터셋 (train)\n",
        "train_dataset = VOCSegWithDepth(root=root, year='2012', image_set='train', download=True,\n",
        "                                  transform_rgb=transform_rgb,\n",
        "                                  transform_depth=transform_depth,\n",
        "                                  transform_target=transform_target)\n",
        "# 테스트 데이터셋 (val)\n",
        "test_dataset = VOCSegWithDepth(root=root, year='2012', image_set='val', download=True,\n",
        "                                 transform_rgb=transform_rgb,\n",
        "                                 transform_depth=transform_depth,\n",
        "                                 transform_target=transform_target)\n",
        "\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psVWkOkd2UzW",
        "outputId": "e593aab0-112e-49c8-9dbe-ab4d0dac1684"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.00G/2.00G [00:24<00:00, 82.1MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# 4. 간단한 FuseNet 모델 구현 (Simplified Version)\n",
        "# ---------------------------------\n",
        "# 기본 구성 블록: Conv -> BatchNorm -> ReLU (CBR)\n",
        "class CBR(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(CBR, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
        "        self.bn   = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "# Encoder branch: 간단하게 한 블록 사용 (CBR + MaxPool)\n",
        "class EncoderBranch(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(EncoderBranch, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            CBR(in_channels, 64),\n",
        "            CBR(64, 64)\n",
        "        )\n",
        "        self.pool  = nn.MaxPool2d(2,2)  # 다운샘플링\n",
        "    def forward(self, x):\n",
        "        features = self.block(x)\n",
        "        pooled   = self.pool(features)\n",
        "        return features, pooled\n",
        "\n",
        "# 간단한 디코더: 업샘플링(Interpolation) + CBR 블록 + 1x1 Convolution으로 예측 채널 맞춤\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            CBR(64, 64),\n",
        "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        )\n",
        "    def forward(self, x, output_size):\n",
        "        # 단순하게 bilinear interpolation으로 업샘플링\n",
        "        x = nn.functional.interpolate(x, size=output_size, mode='bilinear', align_corners=True)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "# FuseNet: 두 개의 branch (RGB, Depth) → Fusion → Decoder\n",
        "class SimpleFuseNet(nn.Module):\n",
        "    def __init__(self, num_classes=21):\n",
        "        super(SimpleFuseNet, self).__init__()\n",
        "        self.rgb_encoder = EncoderBranch(in_channels=3)\n",
        "        self.depth_encoder = EncoderBranch(in_channels=1)\n",
        "        # 여기서는 단순 fusion: 각 branch의 block 출력을 element-wise sum\n",
        "        self.decoder = Decoder(num_classes)\n",
        "    def forward(self, rgb, depth):\n",
        "        # 인코더 각각에서 feature 추출\n",
        "        rgb_features, _ = self.rgb_encoder(rgb)       # (batch, 64, H', W')\n",
        "        depth_features, _ = self.depth_encoder(depth)   # (batch, 64, H', W')\n",
        "        # Fusion: element-wise summation\n",
        "        fused = rgb_features + depth_features\n",
        "        # 디코더: 업샘플링. 원래 입력 이미지 해상도 (224,224)로 복원\n",
        "        out = self.decoder(fused, output_size=rgb.size()[2:])\n",
        "        return out"
      ],
      "metadata": {
        "id": "YkkCMYHK2a73"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# 5. 학습 및 평가 준비\n",
        "# ---------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 21  # VOC Segmentation의 경우 21개 클래스 (0 ~ 20)\n",
        "\n",
        "model = SimpleFuseNet(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)  # 255를 무시 (VOC 라벨 기준)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)"
      ],
      "metadata": {
        "id": "6BNF8kRB2npT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# 6. 학습 루프\n",
        "# ---------------------------------\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (rgb, depth, target) in enumerate(train_loader):\n",
        "        rgb = rgb.to(device)\n",
        "        depth = depth.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(rgb, depth)  # 출력: (batch, num_classes, H, W)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SPIBekJX2qEO",
        "outputId": "a6a50695-d71e-45a5-852f-782d55318dda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/366], Loss: 2.6160\n",
            "Epoch [1/10], Step [20/366], Loss: 2.2132\n",
            "Epoch [1/10], Step [30/366], Loss: 1.6581\n",
            "Epoch [1/10], Step [40/366], Loss: 1.2841\n",
            "Epoch [1/10], Step [50/366], Loss: 1.8149\n",
            "Epoch [1/10], Step [60/366], Loss: 1.0158\n",
            "Epoch [1/10], Step [70/366], Loss: 1.7720\n",
            "Epoch [1/10], Step [80/366], Loss: 0.8940\n",
            "Epoch [1/10], Step [90/366], Loss: 1.2676\n",
            "Epoch [1/10], Step [100/366], Loss: 1.6330\n",
            "Epoch [1/10], Step [110/366], Loss: 0.8756\n",
            "Epoch [1/10], Step [120/366], Loss: 0.7867\n",
            "Epoch [1/10], Step [130/366], Loss: 0.6887\n",
            "Epoch [1/10], Step [140/366], Loss: 1.1580\n",
            "Epoch [1/10], Step [150/366], Loss: 1.4648\n",
            "Epoch [1/10], Step [160/366], Loss: 1.6416\n",
            "Epoch [1/10], Step [170/366], Loss: 1.1076\n",
            "Epoch [1/10], Step [180/366], Loss: 1.2719\n",
            "Epoch [1/10], Step [190/366], Loss: 2.6285\n",
            "Epoch [1/10], Step [200/366], Loss: 1.2336\n",
            "Epoch [1/10], Step [210/366], Loss: 0.9260\n",
            "Epoch [1/10], Step [220/366], Loss: 1.2159\n",
            "Epoch [1/10], Step [230/366], Loss: 1.1352\n",
            "Epoch [1/10], Step [240/366], Loss: 0.7431\n",
            "Epoch [1/10], Step [250/366], Loss: 0.7572\n",
            "Epoch [1/10], Step [260/366], Loss: 1.1670\n",
            "Epoch [1/10], Step [270/366], Loss: 1.3890\n",
            "Epoch [1/10], Step [280/366], Loss: 1.6328\n",
            "Epoch [1/10], Step [290/366], Loss: 1.6282\n",
            "Epoch [1/10], Step [300/366], Loss: 1.1712\n",
            "Epoch [1/10], Step [310/366], Loss: 1.2723\n",
            "Epoch [1/10], Step [320/366], Loss: 1.5085\n",
            "Epoch [1/10], Step [330/366], Loss: 0.6716\n",
            "Epoch [1/10], Step [340/366], Loss: 1.3861\n",
            "Epoch [1/10], Step [350/366], Loss: 1.1654\n",
            "Epoch [1/10], Step [360/366], Loss: 1.8543\n",
            "Epoch [1/10] Loss: 1.4515\n",
            "Epoch [2/10], Step [10/366], Loss: 1.0397\n",
            "Epoch [2/10], Step [20/366], Loss: 0.6437\n",
            "Epoch [2/10], Step [30/366], Loss: 1.4114\n",
            "Epoch [2/10], Step [40/366], Loss: 1.6475\n",
            "Epoch [2/10], Step [50/366], Loss: 1.6979\n",
            "Epoch [2/10], Step [60/366], Loss: 1.5782\n",
            "Epoch [2/10], Step [70/366], Loss: 1.0178\n",
            "Epoch [2/10], Step [80/366], Loss: 1.1654\n",
            "Epoch [2/10], Step [90/366], Loss: 1.0755\n",
            "Epoch [2/10], Step [100/366], Loss: 1.6996\n",
            "Epoch [2/10], Step [110/366], Loss: 1.2617\n",
            "Epoch [2/10], Step [120/366], Loss: 1.0401\n",
            "Epoch [2/10], Step [130/366], Loss: 1.6359\n",
            "Epoch [2/10], Step [140/366], Loss: 1.5869\n",
            "Epoch [2/10], Step [150/366], Loss: 1.1250\n",
            "Epoch [2/10], Step [160/366], Loss: 0.9674\n",
            "Epoch [2/10], Step [170/366], Loss: 0.9068\n",
            "Epoch [2/10], Step [180/366], Loss: 1.2531\n",
            "Epoch [2/10], Step [190/366], Loss: 1.2689\n",
            "Epoch [2/10], Step [200/366], Loss: 0.7510\n",
            "Epoch [2/10], Step [210/366], Loss: 0.9139\n",
            "Epoch [2/10], Step [220/366], Loss: 1.5856\n",
            "Epoch [2/10], Step [230/366], Loss: 1.1031\n",
            "Epoch [2/10], Step [240/366], Loss: 0.9858\n",
            "Epoch [2/10], Step [250/366], Loss: 0.7662\n",
            "Epoch [2/10], Step [260/366], Loss: 1.3704\n",
            "Epoch [2/10], Step [270/366], Loss: 1.1178\n",
            "Epoch [2/10], Step [280/366], Loss: 1.3469\n",
            "Epoch [2/10], Step [290/366], Loss: 0.9433\n",
            "Epoch [2/10], Step [300/366], Loss: 1.5290\n",
            "Epoch [2/10], Step [310/366], Loss: 1.0875\n",
            "Epoch [2/10], Step [320/366], Loss: 1.5154\n",
            "Epoch [2/10], Step [330/366], Loss: 1.5976\n",
            "Epoch [2/10], Step [340/366], Loss: 1.0111\n",
            "Epoch [2/10], Step [350/366], Loss: 0.8327\n",
            "Epoch [2/10], Step [360/366], Loss: 0.9823\n",
            "Epoch [2/10] Loss: 1.2840\n",
            "Epoch [3/10], Step [10/366], Loss: 0.6860\n",
            "Epoch [3/10], Step [20/366], Loss: 1.1232\n",
            "Epoch [3/10], Step [30/366], Loss: 1.2065\n",
            "Epoch [3/10], Step [40/366], Loss: 1.0268\n",
            "Epoch [3/10], Step [50/366], Loss: 1.8699\n",
            "Epoch [3/10], Step [60/366], Loss: 0.9876\n",
            "Epoch [3/10], Step [70/366], Loss: 0.8960\n",
            "Epoch [3/10], Step [80/366], Loss: 1.1224\n",
            "Epoch [3/10], Step [90/366], Loss: 0.7240\n",
            "Epoch [3/10], Step [100/366], Loss: 1.5054\n",
            "Epoch [3/10], Step [110/366], Loss: 1.4294\n",
            "Epoch [3/10], Step [120/366], Loss: 0.7745\n",
            "Epoch [3/10], Step [130/366], Loss: 2.2673\n",
            "Epoch [3/10], Step [140/366], Loss: 0.4196\n",
            "Epoch [3/10], Step [150/366], Loss: 1.1218\n",
            "Epoch [3/10], Step [160/366], Loss: 0.9606\n",
            "Epoch [3/10], Step [170/366], Loss: 1.6050\n",
            "Epoch [3/10], Step [180/366], Loss: 1.0434\n",
            "Epoch [3/10], Step [190/366], Loss: 0.6258\n",
            "Epoch [3/10], Step [200/366], Loss: 1.7781\n",
            "Epoch [3/10], Step [210/366], Loss: 1.4161\n",
            "Epoch [3/10], Step [220/366], Loss: 1.7726\n",
            "Epoch [3/10], Step [230/366], Loss: 1.6641\n",
            "Epoch [3/10], Step [240/366], Loss: 1.7244\n",
            "Epoch [3/10], Step [250/366], Loss: 0.8354\n",
            "Epoch [3/10], Step [260/366], Loss: 1.1139\n",
            "Epoch [3/10], Step [270/366], Loss: 0.9044\n",
            "Epoch [3/10], Step [280/366], Loss: 1.5608\n",
            "Epoch [3/10], Step [290/366], Loss: 1.2683\n",
            "Epoch [3/10], Step [300/366], Loss: 1.6120\n",
            "Epoch [3/10], Step [310/366], Loss: 1.3753\n",
            "Epoch [3/10], Step [320/366], Loss: 1.2197\n",
            "Epoch [3/10], Step [330/366], Loss: 1.8076\n",
            "Epoch [3/10], Step [340/366], Loss: 2.1167\n",
            "Epoch [3/10], Step [350/366], Loss: 1.4829\n",
            "Epoch [3/10], Step [360/366], Loss: 0.8584\n",
            "Epoch [3/10] Loss: 1.2646\n",
            "Epoch [4/10], Step [10/366], Loss: 0.7227\n",
            "Epoch [4/10], Step [20/366], Loss: 1.0460\n",
            "Epoch [4/10], Step [30/366], Loss: 1.2769\n",
            "Epoch [4/10], Step [40/366], Loss: 1.8975\n",
            "Epoch [4/10], Step [50/366], Loss: 1.2039\n",
            "Epoch [4/10], Step [60/366], Loss: 0.8334\n",
            "Epoch [4/10], Step [70/366], Loss: 0.9474\n",
            "Epoch [4/10], Step [80/366], Loss: 1.3341\n",
            "Epoch [4/10], Step [90/366], Loss: 0.9622\n",
            "Epoch [4/10], Step [100/366], Loss: 1.7210\n",
            "Epoch [4/10], Step [110/366], Loss: 1.8885\n",
            "Epoch [4/10], Step [120/366], Loss: 1.1666\n",
            "Epoch [4/10], Step [130/366], Loss: 0.6258\n",
            "Epoch [4/10], Step [140/366], Loss: 1.7517\n",
            "Epoch [4/10], Step [150/366], Loss: 1.0020\n",
            "Epoch [4/10], Step [160/366], Loss: 1.2357\n",
            "Epoch [4/10], Step [170/366], Loss: 1.3486\n",
            "Epoch [4/10], Step [180/366], Loss: 1.2313\n",
            "Epoch [4/10], Step [190/366], Loss: 0.6751\n",
            "Epoch [4/10], Step [200/366], Loss: 1.5392\n",
            "Epoch [4/10], Step [210/366], Loss: 0.9440\n",
            "Epoch [4/10], Step [220/366], Loss: 1.9386\n",
            "Epoch [4/10], Step [230/366], Loss: 1.4629\n",
            "Epoch [4/10], Step [240/366], Loss: 2.1986\n",
            "Epoch [4/10], Step [250/366], Loss: 0.9332\n",
            "Epoch [4/10], Step [260/366], Loss: 2.3313\n",
            "Epoch [4/10], Step [270/366], Loss: 1.8505\n",
            "Epoch [4/10], Step [280/366], Loss: 0.9094\n",
            "Epoch [4/10], Step [290/366], Loss: 1.2083\n",
            "Epoch [4/10], Step [300/366], Loss: 0.7937\n",
            "Epoch [4/10], Step [310/366], Loss: 1.9929\n",
            "Epoch [4/10], Step [320/366], Loss: 1.5982\n",
            "Epoch [4/10], Step [330/366], Loss: 1.8340\n",
            "Epoch [4/10], Step [340/366], Loss: 1.4930\n",
            "Epoch [4/10], Step [350/366], Loss: 1.0388\n",
            "Epoch [4/10], Step [360/366], Loss: 1.6270\n",
            "Epoch [4/10] Loss: 1.2551\n",
            "Epoch [5/10], Step [10/366], Loss: 2.1319\n",
            "Epoch [5/10], Step [20/366], Loss: 1.0539\n",
            "Epoch [5/10], Step [30/366], Loss: 0.9871\n",
            "Epoch [5/10], Step [40/366], Loss: 1.4025\n",
            "Epoch [5/10], Step [50/366], Loss: 1.4365\n",
            "Epoch [5/10], Step [60/366], Loss: 0.6751\n",
            "Epoch [5/10], Step [70/366], Loss: 0.6865\n",
            "Epoch [5/10], Step [80/366], Loss: 1.3744\n",
            "Epoch [5/10], Step [90/366], Loss: 2.3216\n",
            "Epoch [5/10], Step [100/366], Loss: 1.6687\n",
            "Epoch [5/10], Step [110/366], Loss: 1.2812\n",
            "Epoch [5/10], Step [120/366], Loss: 1.3404\n",
            "Epoch [5/10], Step [130/366], Loss: 0.8221\n",
            "Epoch [5/10], Step [140/366], Loss: 1.3036\n",
            "Epoch [5/10], Step [150/366], Loss: 0.7269\n",
            "Epoch [5/10], Step [160/366], Loss: 1.2396\n",
            "Epoch [5/10], Step [170/366], Loss: 1.0111\n",
            "Epoch [5/10], Step [180/366], Loss: 1.5773\n",
            "Epoch [5/10], Step [190/366], Loss: 1.1279\n",
            "Epoch [5/10], Step [200/366], Loss: 0.9602\n",
            "Epoch [5/10], Step [210/366], Loss: 1.5099\n",
            "Epoch [5/10], Step [220/366], Loss: 0.7949\n",
            "Epoch [5/10], Step [230/366], Loss: 1.5765\n",
            "Epoch [5/10], Step [240/366], Loss: 1.1000\n",
            "Epoch [5/10], Step [250/366], Loss: 1.0702\n",
            "Epoch [5/10], Step [260/366], Loss: 0.8928\n",
            "Epoch [5/10], Step [270/366], Loss: 0.7479\n",
            "Epoch [5/10], Step [280/366], Loss: 1.6842\n",
            "Epoch [5/10], Step [290/366], Loss: 0.7586\n",
            "Epoch [5/10], Step [300/366], Loss: 0.6728\n",
            "Epoch [5/10], Step [310/366], Loss: 1.0424\n",
            "Epoch [5/10], Step [320/366], Loss: 0.9241\n",
            "Epoch [5/10], Step [330/366], Loss: 0.5791\n",
            "Epoch [5/10], Step [340/366], Loss: 1.8163\n",
            "Epoch [5/10], Step [350/366], Loss: 0.9847\n",
            "Epoch [5/10], Step [360/366], Loss: 2.1918\n",
            "Epoch [5/10] Loss: 1.2425\n",
            "Epoch [6/10], Step [10/366], Loss: 0.7995\n",
            "Epoch [6/10], Step [20/366], Loss: 0.7022\n",
            "Epoch [6/10], Step [30/366], Loss: 0.7171\n",
            "Epoch [6/10], Step [40/366], Loss: 0.9354\n",
            "Epoch [6/10], Step [50/366], Loss: 1.1365\n",
            "Epoch [6/10], Step [60/366], Loss: 1.3699\n",
            "Epoch [6/10], Step [70/366], Loss: 2.0501\n",
            "Epoch [6/10], Step [80/366], Loss: 0.7314\n",
            "Epoch [6/10], Step [90/366], Loss: 0.7702\n",
            "Epoch [6/10], Step [100/366], Loss: 1.4808\n",
            "Epoch [6/10], Step [110/366], Loss: 1.4789\n",
            "Epoch [6/10], Step [120/366], Loss: 1.6906\n",
            "Epoch [6/10], Step [130/366], Loss: 0.9075\n",
            "Epoch [6/10], Step [140/366], Loss: 0.9648\n",
            "Epoch [6/10], Step [150/366], Loss: 1.7775\n",
            "Epoch [6/10], Step [160/366], Loss: 0.9796\n",
            "Epoch [6/10], Step [170/366], Loss: 1.5042\n",
            "Epoch [6/10], Step [180/366], Loss: 1.0273\n",
            "Epoch [6/10], Step [190/366], Loss: 1.1307\n",
            "Epoch [6/10], Step [200/366], Loss: 0.8600\n",
            "Epoch [6/10], Step [210/366], Loss: 1.0525\n",
            "Epoch [6/10], Step [220/366], Loss: 1.0156\n",
            "Epoch [6/10], Step [230/366], Loss: 1.2585\n",
            "Epoch [6/10], Step [240/366], Loss: 0.9509\n",
            "Epoch [6/10], Step [250/366], Loss: 0.9101\n",
            "Epoch [6/10], Step [260/366], Loss: 0.7472\n",
            "Epoch [6/10], Step [270/366], Loss: 1.1262\n",
            "Epoch [6/10], Step [280/366], Loss: 1.1825\n",
            "Epoch [6/10], Step [290/366], Loss: 0.5774\n",
            "Epoch [6/10], Step [300/366], Loss: 1.5432\n",
            "Epoch [6/10], Step [310/366], Loss: 1.2202\n",
            "Epoch [6/10], Step [320/366], Loss: 0.8169\n",
            "Epoch [6/10], Step [330/366], Loss: 1.3643\n",
            "Epoch [6/10], Step [340/366], Loss: 1.0341\n",
            "Epoch [6/10], Step [350/366], Loss: 0.5981\n",
            "Epoch [6/10], Step [360/366], Loss: 1.5297\n",
            "Epoch [6/10] Loss: 1.2395\n",
            "Epoch [7/10], Step [10/366], Loss: 1.0977\n",
            "Epoch [7/10], Step [20/366], Loss: 0.7573\n",
            "Epoch [7/10], Step [30/366], Loss: 1.5137\n",
            "Epoch [7/10], Step [40/366], Loss: 0.6977\n",
            "Epoch [7/10], Step [50/366], Loss: 1.1014\n",
            "Epoch [7/10], Step [60/366], Loss: 1.1451\n",
            "Epoch [7/10], Step [70/366], Loss: 1.1818\n",
            "Epoch [7/10], Step [80/366], Loss: 1.3209\n",
            "Epoch [7/10], Step [90/366], Loss: 1.2999\n",
            "Epoch [7/10], Step [100/366], Loss: 0.7138\n",
            "Epoch [7/10], Step [110/366], Loss: 1.1623\n",
            "Epoch [7/10], Step [120/366], Loss: 0.6978\n",
            "Epoch [7/10], Step [130/366], Loss: 1.4451\n",
            "Epoch [7/10], Step [140/366], Loss: 0.8722\n",
            "Epoch [7/10], Step [150/366], Loss: 1.0092\n",
            "Epoch [7/10], Step [160/366], Loss: 1.4356\n",
            "Epoch [7/10], Step [170/366], Loss: 2.0965\n",
            "Epoch [7/10], Step [180/366], Loss: 0.4970\n",
            "Epoch [7/10], Step [190/366], Loss: 2.3202\n",
            "Epoch [7/10], Step [200/366], Loss: 1.2344\n",
            "Epoch [7/10], Step [210/366], Loss: 1.1255\n",
            "Epoch [7/10], Step [220/366], Loss: 1.4823\n",
            "Epoch [7/10], Step [230/366], Loss: 1.7334\n",
            "Epoch [7/10], Step [240/366], Loss: 2.0062\n",
            "Epoch [7/10], Step [250/366], Loss: 1.3670\n",
            "Epoch [7/10], Step [260/366], Loss: 0.9686\n",
            "Epoch [7/10], Step [270/366], Loss: 1.6961\n",
            "Epoch [7/10], Step [280/366], Loss: 1.0573\n",
            "Epoch [7/10], Step [290/366], Loss: 1.8510\n",
            "Epoch [7/10], Step [300/366], Loss: 0.8807\n",
            "Epoch [7/10], Step [310/366], Loss: 1.3899\n",
            "Epoch [7/10], Step [320/366], Loss: 0.8269\n",
            "Epoch [7/10], Step [330/366], Loss: 1.4194\n",
            "Epoch [7/10], Step [340/366], Loss: 0.6938\n",
            "Epoch [7/10], Step [350/366], Loss: 1.3715\n",
            "Epoch [7/10], Step [360/366], Loss: 2.3125\n",
            "Epoch [7/10] Loss: 1.2349\n",
            "Epoch [8/10], Step [10/366], Loss: 1.1745\n",
            "Epoch [8/10], Step [20/366], Loss: 1.4150\n",
            "Epoch [8/10], Step [30/366], Loss: 1.4172\n",
            "Epoch [8/10], Step [40/366], Loss: 0.9868\n",
            "Epoch [8/10], Step [50/366], Loss: 0.7843\n",
            "Epoch [8/10], Step [60/366], Loss: 1.3657\n",
            "Epoch [8/10], Step [70/366], Loss: 1.4160\n",
            "Epoch [8/10], Step [80/366], Loss: 1.1672\n",
            "Epoch [8/10], Step [90/366], Loss: 0.9685\n",
            "Epoch [8/10], Step [100/366], Loss: 1.7437\n",
            "Epoch [8/10], Step [110/366], Loss: 2.0301\n",
            "Epoch [8/10], Step [120/366], Loss: 1.3163\n",
            "Epoch [8/10], Step [130/366], Loss: 0.6008\n",
            "Epoch [8/10], Step [140/366], Loss: 0.9873\n",
            "Epoch [8/10], Step [150/366], Loss: 0.5426\n",
            "Epoch [8/10], Step [160/366], Loss: 0.9971\n",
            "Epoch [8/10], Step [170/366], Loss: 1.3248\n",
            "Epoch [8/10], Step [180/366], Loss: 1.2415\n",
            "Epoch [8/10], Step [190/366], Loss: 0.7622\n",
            "Epoch [8/10], Step [200/366], Loss: 1.4190\n",
            "Epoch [8/10], Step [210/366], Loss: 0.8974\n",
            "Epoch [8/10], Step [220/366], Loss: 0.8157\n",
            "Epoch [8/10], Step [230/366], Loss: 0.6011\n",
            "Epoch [8/10], Step [240/366], Loss: 1.9774\n",
            "Epoch [8/10], Step [250/366], Loss: 1.1399\n",
            "Epoch [8/10], Step [260/366], Loss: 1.5891\n",
            "Epoch [8/10], Step [270/366], Loss: 1.0025\n",
            "Epoch [8/10], Step [280/366], Loss: 1.0613\n",
            "Epoch [8/10], Step [290/366], Loss: 1.7085\n",
            "Epoch [8/10], Step [300/366], Loss: 1.0837\n",
            "Epoch [8/10], Step [310/366], Loss: 1.2914\n",
            "Epoch [8/10], Step [320/366], Loss: 1.0539\n",
            "Epoch [8/10], Step [330/366], Loss: 1.2999\n",
            "Epoch [8/10], Step [340/366], Loss: 1.5748\n",
            "Epoch [8/10], Step [350/366], Loss: 0.8837\n",
            "Epoch [8/10], Step [360/366], Loss: 0.8934\n",
            "Epoch [8/10] Loss: 1.2309\n",
            "Epoch [9/10], Step [10/366], Loss: 0.9069\n",
            "Epoch [9/10], Step [20/366], Loss: 1.0088\n",
            "Epoch [9/10], Step [30/366], Loss: 1.4692\n",
            "Epoch [9/10], Step [40/366], Loss: 1.2415\n",
            "Epoch [9/10], Step [50/366], Loss: 0.6709\n",
            "Epoch [9/10], Step [60/366], Loss: 0.9644\n",
            "Epoch [9/10], Step [70/366], Loss: 1.1237\n",
            "Epoch [9/10], Step [80/366], Loss: 1.1533\n",
            "Epoch [9/10], Step [90/366], Loss: 1.3458\n",
            "Epoch [9/10], Step [100/366], Loss: 1.3856\n",
            "Epoch [9/10], Step [110/366], Loss: 1.2650\n",
            "Epoch [9/10], Step [120/366], Loss: 1.2702\n",
            "Epoch [9/10], Step [130/366], Loss: 1.6398\n",
            "Epoch [9/10], Step [140/366], Loss: 1.4519\n",
            "Epoch [9/10], Step [150/366], Loss: 1.2602\n",
            "Epoch [9/10], Step [160/366], Loss: 1.3701\n",
            "Epoch [9/10], Step [170/366], Loss: 1.3081\n",
            "Epoch [9/10], Step [180/366], Loss: 1.6261\n",
            "Epoch [9/10], Step [190/366], Loss: 0.9849\n",
            "Epoch [9/10], Step [200/366], Loss: 0.7845\n",
            "Epoch [9/10], Step [210/366], Loss: 1.4667\n",
            "Epoch [9/10], Step [220/366], Loss: 1.6357\n",
            "Epoch [9/10], Step [230/366], Loss: 1.3418\n",
            "Epoch [9/10], Step [240/366], Loss: 1.0368\n",
            "Epoch [9/10], Step [250/366], Loss: 1.2258\n",
            "Epoch [9/10], Step [260/366], Loss: 1.1290\n",
            "Epoch [9/10], Step [270/366], Loss: 1.8676\n",
            "Epoch [9/10], Step [280/366], Loss: 1.4774\n",
            "Epoch [9/10], Step [290/366], Loss: 0.9488\n",
            "Epoch [9/10], Step [300/366], Loss: 2.1515\n",
            "Epoch [9/10], Step [310/366], Loss: 0.9597\n",
            "Epoch [9/10], Step [320/366], Loss: 1.2023\n",
            "Epoch [9/10], Step [330/366], Loss: 1.9291\n",
            "Epoch [9/10], Step [340/366], Loss: 0.9460\n",
            "Epoch [9/10], Step [350/366], Loss: 1.3829\n",
            "Epoch [9/10], Step [360/366], Loss: 0.5832\n",
            "Epoch [9/10] Loss: 1.2268\n",
            "Epoch [10/10], Step [10/366], Loss: 1.4413\n",
            "Epoch [10/10], Step [20/366], Loss: 1.2421\n",
            "Epoch [10/10], Step [30/366], Loss: 0.9528\n",
            "Epoch [10/10], Step [40/366], Loss: 0.8632\n",
            "Epoch [10/10], Step [50/366], Loss: 0.8105\n",
            "Epoch [10/10], Step [60/366], Loss: 0.6856\n",
            "Epoch [10/10], Step [70/366], Loss: 1.1269\n",
            "Epoch [10/10], Step [80/366], Loss: 1.1139\n",
            "Epoch [10/10], Step [90/366], Loss: 0.7552\n",
            "Epoch [10/10], Step [100/366], Loss: 0.6325\n",
            "Epoch [10/10], Step [110/366], Loss: 1.3173\n",
            "Epoch [10/10], Step [120/366], Loss: 0.8920\n",
            "Epoch [10/10], Step [130/366], Loss: 0.7726\n",
            "Epoch [10/10], Step [140/366], Loss: 1.2664\n",
            "Epoch [10/10], Step [150/366], Loss: 1.5967\n",
            "Epoch [10/10], Step [160/366], Loss: 1.8617\n",
            "Epoch [10/10], Step [170/366], Loss: 0.8755\n",
            "Epoch [10/10], Step [180/366], Loss: 1.4583\n",
            "Epoch [10/10], Step [190/366], Loss: 1.3790\n",
            "Epoch [10/10], Step [200/366], Loss: 1.2367\n",
            "Epoch [10/10], Step [210/366], Loss: 2.0509\n",
            "Epoch [10/10], Step [220/366], Loss: 0.9448\n",
            "Epoch [10/10], Step [230/366], Loss: 1.4704\n",
            "Epoch [10/10], Step [240/366], Loss: 2.0198\n",
            "Epoch [10/10], Step [250/366], Loss: 1.1920\n",
            "Epoch [10/10], Step [260/366], Loss: 0.9518\n",
            "Epoch [10/10], Step [270/366], Loss: 0.7664\n",
            "Epoch [10/10], Step [280/366], Loss: 1.1953\n",
            "Epoch [10/10], Step [290/366], Loss: 1.5178\n",
            "Epoch [10/10], Step [300/366], Loss: 1.1148\n",
            "Epoch [10/10], Step [310/366], Loss: 0.9646\n",
            "Epoch [10/10], Step [320/366], Loss: 1.4183\n",
            "Epoch [10/10], Step [330/366], Loss: 1.6526\n",
            "Epoch [10/10], Step [340/366], Loss: 1.6921\n",
            "Epoch [10/10], Step [350/366], Loss: 0.8830\n",
            "Epoch [10/10], Step [360/366], Loss: 1.4645\n",
            "Epoch [10/10] Loss: 1.2177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# 7. 평가 루프 (Global Accuracy 예시)\n",
        "# ---------------------------------\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_pixels = 0\n",
        "with torch.no_grad():\n",
        "    for rgb, depth, target in test_loader:\n",
        "        rgb = rgb.to(device)\n",
        "        depth = depth.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        outputs = model(rgb, depth)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        total_correct += (preds == target).sum().item()\n",
        "        total_pixels += torch.numel(target)\n",
        "\n",
        "print(f\"Test Global Accuracy: {100 * total_correct / total_pixels:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViQ6BjuQ3mn7",
        "outputId": "09628b07-8c6e-4a92-a3bc-b8f7780ed82b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Global Accuracy: 69.27%\n"
          ]
        }
      ]
    }
  ]
}